{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4becfe82-0dc8-4cbf-b8c5-b0bfe695a94e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<img src=\"https://files.training.databricks.com/images/DeltaLake-logo.png\" width=\"80px\"/>\n",
    "\n",
    "# Unifying Structured Streaming with Batch Jobs with Delta Lake\n",
    "\n",
    "In this notebook, we will explore combining streaming and batch processing with a single pipeline. We will begin by defining the following logic:\n",
    "\n",
    "- ingest streaming JSON data from disk and write it to a Delta Lake Table `/activity/Bronze`\n",
    "- perform a Stream-Static Join on the streamed data to add additional geographic data\n",
    "- transform and load the data, saving it out to our Delta Lake Table `/activity/Silver`\n",
    "- summarize the data through aggregation into the Delta Lake Table `/activity/Gold/groupedCounts`\n",
    "- materialize views of our gold table through streaming plots and static queries\n",
    "\n",
    "We will then demonstrate that by writing batches of data back to our bronze table, we can trigger the same logic on newly loaded data and propagate our changes automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e77faf81-268f-4ee6-8107-c524e7829ce7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./Includes/Classroom-Setup\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "91a24e0c-06ef-49a9-936e-bb662c5b09b7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-sandbox\n",
    "## Set up relevant Delta Lake paths\n",
    "\n",
    "These paths will serve as the file locations for our Delta Lake tables.\n",
    "\n",
    "<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Each streaming write has its own checkpoint directory.\n",
    "\n",
    "<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> You cannot write out new Delta files within a repository that contains Delta files. Note that our hierarchy here isolates each Delta table into its own directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e255c933-f6e1-43ee-afc8-c51a6bf14b24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'userhome' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m activityPath \u001b[38;5;241m=\u001b[39m \u001b[43muserhome\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/activity\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m activityBronzePath \u001b[38;5;241m=\u001b[39m activityPath \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Bronze\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m activityBronzeCheckpoint \u001b[38;5;241m=\u001b[39m activityBronzePath \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/checkpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'userhome' is not defined"
     ]
    }
   ],
   "source": [
    "activityPath = userhome + \"/activity\"\n",
    "\n",
    "activityBronzePath = activityPath + \"/Bronze\"\n",
    "activityBronzeCheckpoint = activityBronzePath + \"/checkpoint\"\n",
    "\n",
    "activitySilverPath = activityPath + \"/Silver\"\n",
    "activitySilverCheckpoint = activitySilverPath + \"/checkpoint\"\n",
    "\n",
    "activityGoldPath = activityPath + \"/Gold\"\n",
    "groupedCountPath = activityGoldPath + \"/groupedCount\"\n",
    "groupedCountCheckpoint = groupedCountPath + \"/checkpoint\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4edd6c38-343f-401c-87d7-66abbd7c00bb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Reset Pipeline\n",
    "\n",
    "To reset the pipeline, run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "05d7ba37-8667-4e13-bade-846ed1c38124",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.fs.rm(activityPath, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b108c84a-e082-434a-9991-623bb57d1560",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-sandbox\n",
    "## Datasets Used\n",
    "This notebook will consume cell phone accelerometer data. Records have been downsampled so that the streaming data represents less than 3% of the total data being produced. The remainder will be processed as batches.\n",
    "\n",
    "The following fields are present:\n",
    "\n",
    "- `Index`\n",
    "- `Arrival_Time`\n",
    "- `Creation_Time`\n",
    "- `x`\n",
    "- `y`\n",
    "- `z`\n",
    "- `User`\n",
    "- `Model`\n",
    "- `Device`\n",
    "- `gt`\n",
    "- `geolocation`\n",
    "\n",
    "## Define Schema\n",
    "\n",
    "For streaming jobs, we need to define our schema before we start.\n",
    "\n",
    "<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> We'll reuse this same schema later in the notebook to define our batch processing, which will eliminate the jobs triggered by eliminating a file scan AND enforce the schema that we've defined here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "353f1067-c7fb-430c-86da-23d43fe63403",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructField, StructType, LongType, StringType, DoubleType\n",
    "\n",
    "schema = StructType([\n",
    "  StructField(\"Arrival_Time\",LongType()),\n",
    "  StructField(\"Creation_Time\",LongType()),\n",
    "  StructField(\"Device\",StringType()),\n",
    "  StructField(\"Index\",LongType()),\n",
    "  StructField(\"Model\",StringType()),\n",
    "  StructField(\"User\",StringType()),\n",
    "  StructField(\"geolocation\",StructType([\n",
    "    StructField(\"city\",StringType()),\n",
    "    StructField(\"country\",StringType())\n",
    "  ])),\n",
    "  StructField(\"gt\",StringType()),\n",
    "  StructField(\"id\",LongType()),\n",
    "  StructField(\"x\",DoubleType()),\n",
    "  StructField(\"y\",DoubleType()),\n",
    "  StructField(\"z\",DoubleType())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "43a62d56-886d-49dd-9ab3-c49985cb04a2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-sandbox\n",
    "\n",
    "### Define Streaming Load from Files in Blob\n",
    "\n",
    "Our streaming source directory has 36 JSON files of 5k records each saved in a repository. Here, we'll trigger processing on files one at a time. \n",
    "\n",
    "<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> In a production setting, this same logic would allow us to only read new files written to our source directory. We could define `maxFilesPerTrigger` to control the amount of data we consume with each load, or omit this option to consume all new data on disk since the last time the stream has processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9567f3c0-9433-4bef-8277-ef1f2da6b556",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rawEventsDF = (spark\n",
    "  .readStream\n",
    "  .format(\"json\")\n",
    "  .schema(schema)\n",
    "  .option(\"maxFilesPerTrigger\", 1)\n",
    "  .load(\"/mnt/training/definitive-guide/data/activity-json/streaming\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9d5941a4-1853-45a2-9e79-587c11904ebe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-sandbox\n",
    "### WRITE Stream using Delta Lake\n",
    "\n",
    "#### General Notation\n",
    "Use this format to write a streaming job to a Delta Lake table.\n",
    "\n",
    "<pre>\n",
    "(myDF\n",
    "  .writeStream\n",
    "  .format(\"delta\")\n",
    "  .option(\"checkpointLocation\", checkpointPath)\n",
    "  .outputMode(\"append\")\n",
    "  .start(path)\n",
    ")\n",
    "</pre>\n",
    "\n",
    "<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> While we _can_ write directly to tables using the `.table()` notation, this will create fully managed tables by writing output to a default location on DBFS. This is not best practice for production jobs.\n",
    "\n",
    "#### Output Modes\n",
    "Notice, besides the \"obvious\" parameters, specify `outputMode`, which can take on these values\n",
    "* `append`: add only new records to output sink\n",
    "* `complete`: rewrite full output - applicable to aggregations operations\n",
    "\n",
    "<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> At present, `update` mode is **not** supported for streaming Delta jobs.\n",
    "\n",
    "#### Checkpointing\n",
    "\n",
    "When defining a Delta Lake streaming query, one of the options that you need to specify is the location of a checkpoint directory.\n",
    "\n",
    "`.writeStream.format(\"delta\").option(\"checkpointLocation\", <path-to-checkpoint-directory>) ...`\n",
    "\n",
    "This is actually a structured streaming feature. It stores the current state of your streaming job.\n",
    "\n",
    "Should your streaming job stop for some reason and you restart it, it will continue from where it left off.\n",
    "\n",
    "<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> If you do not have a checkpoint directory, when the streaming job stops, you lose all state around your streaming job and upon restart, you start from scratch.\n",
    "\n",
    "<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> Also note that every streaming job should have its own checkpoint directory: no sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "23c4945f-64c0-40d8-ab49-1ad9a17f3ed8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(rawEventsDF\n",
    "  .writeStream\n",
    "  .format(\"delta\")\n",
    "  .option(\"checkpointLocation\", activityBronzeCheckpoint)\n",
    "  .outputMode(\"append\")\n",
    "  .start(activityBronzePath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3cc69c03-c5e2-4728-a410-7a9353494d73",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Load Static Lookup Table\n",
    "\n",
    "Before enriching our bronze data, we will load a static lookup table for our country codes.\n",
    "\n",
    "Here, we'll use a parquet file that contains countries and their associated codes and abbreviations.\n",
    "\n",
    "While we can load this as a table (which will copy all files to the workspace and make it available to all users), here we'll manipulate it as a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9108b0fd-3fbc-4f5c-8dbb-33a540b4a108",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "geoForLookupDF = (spark\n",
    "  .read\n",
    "  .format(\"parquet\")\n",
    "  .load(\"/mnt/training/countries/ISOCountryCodes/ISOCountryLookup.parquet/\")\n",
    "  .select(col(\"EnglishShortName\").alias(\"country\"), col(\"alpha3Code\").alias(\"countryCode3\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "843714e4-1be6-4c96-b65e-21270c726c1c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-sandbox\n",
    "\n",
    "## Create QUERY tables (aka \"silver tables\")\n",
    "\n",
    "Our current bronze table contains nested fields, as well as time data that has been encoded in non-standard unix time (`Arrival_Time` is encoded as milliseconds from epoch, while `Creation_Time` records nanoseconds between record creation and receipt). \n",
    "\n",
    "We also wish to enrich our data with 3 letter country codes for mapping purposes, which we'll obtain from a join with our `geoForLookupDF`.\n",
    "\n",
    "In order to parse the data in human-readable form, we create query/silver tables out of the raw data.\n",
    "\n",
    "We will stream from our previous file write, define transformations, and rewrite our data to disk.\n",
    "\n",
    "<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Notice how we do not need to specify a schema when loading Delta files: it is inferred from the metadata!\n",
    "\n",
    "The fields of a complex object can be referenced with a \"dot\" notation as in:\n",
    "\n",
    "`col(\"geolocation.country\")`\n",
    "\n",
    "\n",
    "A large number of these fields/columns can become unwieldy.\n",
    "\n",
    "For that reason, it is common to extract the sub-fields and represent them as first-level columns as seen below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "302aef5a-4e23-4350-a5fa-1556d79f7438",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "NOTE: You will not be able to run this command until the `rawEventsDF` has initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "aee4bbd2-9088-4dbf-8afe-94d40080c04c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_unixtime\n",
    "\n",
    "parsedEventsDF = (spark.readStream\n",
    "  .format(\"delta\")\n",
    "  .load(activityBronzePath)\n",
    "  .select(from_unixtime(col(\"Arrival_Time\")/1000).alias(\"Arrival_Time\").cast(\"timestamp\"),\n",
    "          (col(\"Creation_Time\")/1E9).alias(\"Creation_Time\").cast(\"timestamp\"),\n",
    "          col(\"Device\"),\n",
    "          col(\"Index\"),\n",
    "          col(\"Model\"),\n",
    "          col(\"User\"),\n",
    "          col(\"gt\"),\n",
    "          col(\"x\"),\n",
    "          col(\"y\"),\n",
    "          col(\"z\"),\n",
    "          col(\"geolocation.country\").alias(\"country\"),\n",
    "          col(\"geolocation.city\").alias(\"city\"))\n",
    "  .join(geoForLookupDF, [\"country\"], \"left\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a483f331-b2c7-4c86-956d-0d79c704efa7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Write to QUERY Tables (aka \"silver tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8f613a06-f59f-4677-a9da-3af89db3a882",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(parsedEventsDF\n",
    "  .writeStream\n",
    "  .format(\"delta\")\n",
    "  .option(\"checkpointLocation\", activitySilverCheckpoint)\n",
    "  .outputMode(\"append\")\n",
    "  .start(activitySilverPath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "157a53c9-a8eb-4139-a4c6-e0b6a1ad32cf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "See contents of Silver directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "078ac29f-b3e3-47b1-9113-55c590234e41",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.fs.ls(activitySilverPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3b91418c-02ef-4afe-aaea-a88e89d7c791",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-sandbox\n",
    "#### See list of active streams.\n",
    "\n",
    "<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> You should currently see two active streams, one for each streaming write that you've triggered. If you have called `display` on either of your streaming DataFrames, you will see an additional stream, as `display` writes the stream to memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c9ed342d-e667-4e7b-8c2b-ead7ad80dde3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for s in spark.streams.active:\n",
    "  print(s.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e0fe5a18-db55-47e4-a425-65007e0815c0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-sandbox\n",
    "### Gold Table: Grouped Count of Events\n",
    "\n",
    "Here we read a stream of data from `activitySilverPath` and write another stream to `activityGoldPath/groupedCount`.\n",
    "\n",
    "The data consists of a total counts of all event, grouped by `hour`, `gt`, and `countryCode3`.\n",
    "\n",
    "Performing this aggregation allows us to reduce the total number of rows in our table from hundreds of thousands (or millions, once we've loaded our batch data) to dozens.\n",
    "\n",
    "In cell cmd25 this can be seen as a materialized view of the streaming data.\n",
    "\n",
    "<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Notice that we're writing to a named directory within our gold path. If we wish to define additional aggregations, we would organize these parallel to thie directory to avoid metadata write conflicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "407dab84-3485-481c-80d3-38c309ff9fa5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import window, hour\n",
    "\n",
    "(spark.readStream\n",
    "  .format(\"delta\")\n",
    "  .load(activitySilverPath)\n",
    "  .groupBy(window(\"Arrival_Time\", \"60 minute\"),\"gt\", \"countryCode3\")\n",
    "  .count()\n",
    "  .withColumn(\"hour\",hour(col(\"window.start\")))\n",
    "  .drop(\"window\")\n",
    "  .writeStream\n",
    "  .format(\"delta\")\n",
    "  .option(\"checkpointLocation\", groupedCountCheckpoint)\n",
    "  .outputMode(\"complete\")\n",
    "  .start(groupedCountPath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "21cf244b-c548-406a-8aac-dec8b1610b53",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### CREATE A Table Using Delta Lake\n",
    "\n",
    "Create a table called `groupedCountPath` using `DELTA` out of the above data.\n",
    "\n",
    "NOTE: You will not be able to run this command until the `groupedCountPath` has initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c576bbc7-e438-45d7-8f26-45677690749e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "  DROP TABLE IF EXISTS grouped_count\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "  CREATE TABLE grouped_count\n",
    "  USING DELTA\n",
    "  LOCATION '{}'\n",
    "\"\"\".format(groupedCountPath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9191016b-c9a4-4f4f-8295-b204c22bedf6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-sandbox\n",
    "#### Important Considerations for `complete` Output with Delta\n",
    "\n",
    "When using `complete` output mode, we rewrite the entire state of our table each time our logic runs. While this is ideal for calculating aggregates, we **cannot** read a stream from this directory, as Structured Streaming assumes data is only being appended in the upstream logic.\n",
    "\n",
    "<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Certain options can be set to change this behavior, but have other limitations attached. For more details, refer to [Delta Streaming: Ignoring Updates and Deletes](https://docs.databricks.com/delta/delta-streaming.html#ignoring-updates-and-deletes).\n",
    "\n",
    "The gold Delta table we have just registered will perform a static read of the current state of the data each time we run the following query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f9cd395e-796e-4646-b1c2-e9ddab846bfd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT * FROM grouped_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8858899a-3b7d-4318-8fbf-149abaa71aec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-sandbox\n",
    "### Materialized View: Windowed Count of Hourly `gt` Events\n",
    "\n",
    "Plot the occurrence of all events grouped by `gt`.\n",
    "\n",
    "<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> Because we're using `complete` output mode for our gold table write, we cannot define a streaming plot on these files.\n",
    "\n",
    "Instead, we'll define a temp table based on the files written to our silver table as shown in the cell cmd cmd 31. We will them use this table to execute our streaming queries.\n",
    "\n",
    "In order to create a LIVE bar chart of the data, you'll need to fill out the <b>Plot Options</b> as shown in cell cmd32 by clicking on the chart icon:\n",
    "\n",
    "<div><img src=\"https://files.training.databricks.com/images/eLearning/Delta/ch5-plot-options.png\"/></div><br/>\n",
    "\n",
    "### Note on Gold Tables & Materialized Views\n",
    "\n",
    "When we call `display` on a streaming DataFrame or execute a SQL query on a streaming view, we are using memory as our sink. \n",
    "\n",
    "In this case, we are executing a SQL query on a streaming view. We have already calculated all the values necessary to materialize our streaming view above in the gold table we've written to disk. \n",
    "\n",
    "**However**, we re-execute this logic on our silver table to generate streaming views, as structured streaming will not support reads from upstream files that have beem overwritten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cbbe5d00-651c-4dd4-9792-105ed059ce05",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(spark.readStream\n",
    "  .format(\"delta\")\n",
    "  .load(activitySilverPath)\n",
    "  .createOrReplaceTempView(\"query_table\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c47b1b54-e9c6-42a4-8425-b2d3c665de0b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT gt, HOUR(Arrival_Time) hour, COUNT(*) total_events\n",
    "FROM query_table\n",
    "GROUP BY gt, HOUR(Arrival_Time)\n",
    "ORDER BY hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0a877ff4-b3d4-442d-a6be-6cf096558699",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Batch Load Data into Bronze Table\n",
    "\n",
    "We can use the same pipeline to process batch data.\n",
    "\n",
    "By loading our raw data into our bronze table, we will push it through our already running streaming logic.\n",
    "\n",
    "Here, we'll run 4 batches of around 170k records. We can track each batch through our streaming plots above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a71ca634-a90f-460d-aab9-073112770f7e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for batch in range(4):\n",
    "  (spark\n",
    "    .read\n",
    "    .format(\"json\")\n",
    "    .schema(schema)\n",
    "    .load(\"/mnt/training/definitive-guide/data/activity-json/batch-{}\".format(batch))\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .save(activityBronzePath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9ab7773a-a5da-4c34-8a46-6f6390b9c238",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Note that even on our small cluster, we can pass a batch of over 5 million records through our logic above without problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "762ab46e-e7d7-44cb-98f2-f858c9b1d3e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(spark\n",
    "  .read\n",
    "  .format(\"json\")\n",
    "  .schema(schema)\n",
    "  .load(\"/mnt/training/definitive-guide/data/activity-json/batch\")\n",
    "  .write\n",
    "  .format(\"delta\")\n",
    "  .mode(\"append\")\n",
    "  .save(activityBronzePath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "41896a13-c726-4022-a8ec-a15e4c23b42b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-sandbox\n",
    "\n",
    "<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> While our streaming materialized view above updates as data flows in, we can also easily generate this view from our `grouped_count` table. \n",
    "\n",
    "We will need to re-run this query each time we wish to update the data. Run the below query now, and then after your batch has finished processing.\n",
    "\n",
    "<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> The state reflected in a query on a registered Delta table will always reflect the most recent valid state of the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "89874d37-19da-423a-958e-1abab9c0471c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT * FROM grouped_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "016c6718-0da1-4255-8034-0319c6ab255c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Wrapping Up\n",
    "\n",
    "Finally, make sure all streams are stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d203631d-10c5-4246-92ae-fe368eab9244",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for s in spark.streams.active:\n",
    "    s.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "747aec8c-84d1-425f-8a9b-6e0df051ff56",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "Delta Lake is ideally suited for use in streaming data lake contexts.\n",
    "\n",
    "Use the Delta Lake architecture to craft raw, query, and summary tables to produce beautiful visualizations of key business metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "63897683-ca7c-4883-88dc-497877db3b42",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Additional Topics & Resources\n",
    "\n",
    "* <a href=\"https://docs.databricks.com/delta/delta-streaming.html#as-a-sink\" target=\"_blank\">Delta Streaming Write Notation</a>\n",
    "* <a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#\" target=\"_blank\">Structured Streaming Programming Guide</a>\n",
    "* <a href=\"https://www.youtube.com/watch?v=rl8dIzTpxrI\" target=\"_blank\">A Deep Dive into Structured Streaming</a> by Tagatha Das. This is an excellent video describing how Structured Streaming works.\n",
    "* <a href=\"http://lambda-architecture.net/#\" target=\"_blank\">Lambda Architecture</a>\n",
    "* <a href=\"https://bennyaustin.wordpress.com/2010/05/02/kimball-and-inmon-dw-models/#\" target=\"_blank\">Data Warehouse Models</a>\n",
    "* <a href=\"https://people.apache.org//~pwendell/spark-nightly/spark-branch-2.1-docs/latest/structured-streaming-kafka-integration.html#\" target=\"_blank\">Reading structured streams from Kafka</a>\n",
    "* <a href=\"http://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html#creating-a-kafka-source-stream#\" target=\"_blank\">Create a Kafka Source Stream</a>\n",
    "* <a href=\"https://docs.databricks.com/delta/delta-intro.html#case-study-multi-hop-pipelines#\" target=\"_blank\">Multi Hop Pipelines</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "1-Delta-Architecture",
   "notebookOrigID": 2553845735016192,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
